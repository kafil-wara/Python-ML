{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "6S1I_DsCbtRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgZd6c6xX1TV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365c34ee-c015-4369-9aeb-28846e78bab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of positive tweets: 5000\n",
            "Total no of negative tweets: 5000\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "#nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "#print(positive_tweets)\n",
        "print(\"Total no of positive tweets: \" + str(len(positive_tweets)))\n",
        "print(\"Total no of negative tweets: \" + str(len(positive_tweets)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAXjYJTxbUlB",
        "outputId": "62d95126-ea2b-4f3a-e525-237b2b794bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
              " '@97sides CONGRATS :)',\n",
              " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n",
              " '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM',\n",
              " \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\",\n",
              " '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.',\n",
              " 'Jgh , but we have to go to Bayan :D bye',\n",
              " 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_tweets[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iq9jQU0bfyZ",
        "outputId": "fcc45921-ad38-4d9b-a479-fbf5c6ba1345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hopeless for tmr :(',\n",
              " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
              " '@Hegelbon That heart sliding into the waste basket. :(',\n",
              " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
              " 'Dang starting next week I have \"work\" :(',\n",
              " \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\",\n",
              " '@RileyMcDonough make me smile :((',\n",
              " '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln',\n",
              " 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"',\n",
              " 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "mD2tiENocG9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "nltk.download('punkt')\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "\n",
        "print(tweet_tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd3AF00ySqsk",
        "outputId": "43f31993-c09a-47af-82de-5103b66d026e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "print(pos_tag(tweet_tokens[7]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NON_eDvYSq0I",
        "outputId": "ebd5101e-2f47-4116-c149-7543e6d5d33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('@Impatientraider', 'NN'), ('On', 'IN'), ('second', 'JJ'), ('thought', 'NN'), (',', ','), ('there', 'EX'), ('’', 'NNP'), ('s', 'NN'), ('just', 'RB'), ('not', 'RB'), ('enough', 'JJ'), ('time', 'NN'), ('for', 'IN'), ('a', 'DT'), ('DD', 'NNP'), (':)', 'NNP'), ('But', 'CC'), ('new', 'JJ'), ('shorts', 'NNS'), ('entering', 'VBG'), ('system', 'NN'), ('.', '.'), ('Sheep', 'VB'), ('must', 'MD'), ('be', 'VB'), ('buying', 'VBG'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence\n",
        "\n",
        "print(lemmatize_sentence(tweet_tokens[7]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSMD9Fs6Sq1i",
        "outputId": "51f4c70f-b4e3-48bf-a4f6-53c74ab821d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@Impatientraider', 'On', 'second', 'thought', ',', 'there', '’', 's', 'just', 'not', 'enough', 'time', 'for', 'a', 'DD', ':)', 'But', 'new', 'short', 'enter', 'system', '.', 'Sheep', 'must', 'be', 'buy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "6cOcZpEFSq8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "print(positive_tweet_tokens[7])\n",
        "print(positive_cleaned_tokens_list[7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2EsGEUXSrC8",
        "outputId": "c8b6a0b1-faaf-49e0-9719-c8013c5a3af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@Impatientraider', 'On', 'second', 'thought', ',', 'there', '’', 's', 'just', 'not', 'enough', 'time', 'for', 'a', 'DD', ':)', 'But', 'new', 'shorts', 'entering', 'system', '.', 'Sheep', 'must', 'be', 'buying', '.']\n",
            "['second', 'thought', '’', 'enough', 'time', 'dd', ':)', 'new', 'short', 'enter', 'system', 'sheep', 'must', 'buy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
        "\n",
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOm-Eg4VdXzI",
        "outputId": "1b5f16da-a17e-48c3-bc0f-e3647375bf18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
        "\n",
        "import random\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]\n"
      ],
      "metadata": {
        "id": "o-nD6JCSdYLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models**"
      ],
      "metadata": {
        "id": "_kjlrPDucUpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "UgzBxCNEceUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "naivebayes = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(naivebayes, test_data))\n",
        "\n",
        "print(naivebayes.show_most_informative_features(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceuuZ6ZcBdzJ",
        "outputId": "eb07bb46-0ba6-44d0-c5a3-fd2e48876aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.997\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2065.8 : 1.0\n",
            "                follower = True           Positi : Negati =     22.5 : 1.0\n",
            "                     sad = True           Negati : Positi =     19.3 : 1.0\n",
            "                    sick = True           Negati : Positi =     18.4 : 1.0\n",
            "                    cool = True           Positi : Negati =     18.2 : 1.0\n",
            "                     bam = True           Positi : Negati =     17.6 : 1.0\n",
            "                     x15 = True           Negati : Positi =     17.1 : 1.0\n",
            "                     via = True           Positi : Negati =     16.1 : 1.0\n",
            "                    blog = True           Positi : Negati =     15.6 : 1.0\n",
            "                followed = True           Negati : Positi =     15.5 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree**"
      ],
      "metadata": {
        "id": "xPjSVkjzcopH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import DecisionTreeClassifier\n",
        "decisiontree = DecisionTreeClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(decisiontree, test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHiaNXi3j4jm",
        "outputId": "2ebed07d-b7a4-4d63-9c16-53bef149673b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing on Custom Data**"
      ],
      "metadata": {
        "id": "IcU3iUaZcuhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "custom_tweet = \"Used foodpanda today, great service! Recommended\"\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(naivebayes.classify(dict([token, True] for token in custom_tokens)))\n",
        "print(decisiontree.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9otXIixdYOe",
        "outputId": "cd0b365d-099c-4236-df73-bab60fe1c17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n",
            "Positive\n"
          ]
        }
      ]
    }
  ]
}